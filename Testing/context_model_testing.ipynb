{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, PreProcessor, FARMReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\ANT-PC\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.26it/s]\n"
     ]
    }
   ],
   "source": [
    "squad_dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = squad_dataset[\"train\"]\n",
    "validation  = squad_dataset[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_document = []\n",
    "\n",
    "for data in train:\n",
    "    tmp_doc = Document(content=data[\"context\"])\n",
    "    tmp_doc.id = data['id']\n",
    "    tmp_doc.content_type = \"str\"\n",
    "    train_document.append(tmp_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing Documents: 90000it [01:39, 904.12it/s]                           \n"
     ]
    }
   ],
   "source": [
    "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")\n",
    "document_store.write_documents(train_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find facebook/dpr-question_encoder-single-nq-base locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded facebook/dpr-question_encoder-single-nq-base\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find facebook/dpr-ctx_encoder-single-nq-base locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded facebook/dpr-ctx_encoder-single-nq-base\n",
      "INFO - haystack.document_stores.faiss -  Updating embeddings for 87599 docs...\n",
      "Documents Processed:  34%|███▍      | 30000/87599 [37:42<1:12:24, 13.26 docs/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Website-Chatbot\\Testing\\context_model_testing.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Website-Chatbot/Testing/context_model_testing.ipynb#ch0000006?line=0'>1</a>\u001b[0m retriever \u001b[39m=\u001b[39m DensePassageRetriever(document_store\u001b[39m=\u001b[39mdocument_store,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Website-Chatbot/Testing/context_model_testing.ipynb#ch0000006?line=1'>2</a>\u001b[0m                                  query_embedding_model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfacebook/dpr-question_encoder-single-nq-base\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Website-Chatbot/Testing/context_model_testing.ipynb#ch0000006?line=2'>3</a>\u001b[0m                                  passage_embedding_model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfacebook/dpr-ctx_encoder-single-nq-base\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Website-Chatbot/Testing/context_model_testing.ipynb#ch0000006?line=8'>9</a>\u001b[0m                                  use_fast_tokenizers\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Website-Chatbot/Testing/context_model_testing.ipynb#ch0000006?line=9'>10</a>\u001b[0m                                  similarity_function\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Website-Chatbot/Testing/context_model_testing.ipynb#ch0000006?line=10'>11</a>\u001b[0m document_store\u001b[39m.\u001b[39;49mupdate_embeddings(retriever)\n",
      "File \u001b[1;32mc:\\Users\\ANT-PC\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\document_stores\\faiss.py:358\u001b[0m, in \u001b[0;36mFAISSDocumentStore.update_embeddings\u001b[1;34m(self, retriever, index, update_existing_embeddings, filters, batch_size)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(\n\u001b[0;32m    355\u001b[0m     total\u001b[39m=\u001b[39mdocument_count, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m docs\u001b[39m\u001b[39m\"\u001b[39m, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUpdating Embedding\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    356\u001b[0m ) \u001b[39mas\u001b[39;00m progress_bar:\n\u001b[0;32m    357\u001b[0m     \u001b[39mfor\u001b[39;00m document_batch \u001b[39min\u001b[39;00m batched_documents:\n\u001b[1;32m--> 358\u001b[0m         embeddings \u001b[39m=\u001b[39m retriever\u001b[39m.\u001b[39;49membed_documents(document_batch)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    359\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(document_batch) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(embeddings)\n\u001b[0;32m    361\u001b[0m         embeddings_to_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(embeddings, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ANT-PC\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\nodes\\retriever\\dense.py:348\u001b[0m, in \u001b[0;36mDensePassageRetriever.embed_documents\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor\u001b[39m.\u001b[39mnum_hard_negatives \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    335\u001b[0m passages \u001b[39m=\u001b[39m [\n\u001b[0;32m    336\u001b[0m     {\n\u001b[0;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpassages\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m docs\n\u001b[0;32m    347\u001b[0m ]\n\u001b[1;32m--> 348\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_predictions(passages)[\u001b[39m\"\u001b[39m\u001b[39mpassages\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    350\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mc:\\Users\\ANT-PC\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\nodes\\retriever\\dense.py:301\u001b[0m, in \u001b[0;36mDensePassageRetriever._get_predictions\u001b[1;34m(self, dicts)\u001b[0m\n\u001b[0;32m    299\u001b[0m                 all_embeddings[\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(query_embeddings\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m    300\u001b[0m             \u001b[39mif\u001b[39;00m passage_embeddings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 301\u001b[0m                 all_embeddings[\u001b[39m\"\u001b[39m\u001b[39mpassages\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(passage_embeddings\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m    302\u001b[0m         progress_bar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m    304\u001b[0m \u001b[39mif\u001b[39;00m all_embeddings[\u001b[39m\"\u001b[39m\u001b[39mpassages\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(document_store=document_store,\n",
    "                                 query_embedding_model='facebook/dpr-question_encoder-single-nq-base',\n",
    "                                 passage_embedding_model='facebook/dpr-ctx_encoder-single-nq-base',\n",
    "                                 max_seq_len_query=64,\n",
    "                                 max_seq_len_passage=512,\n",
    "                                 batch_size=16,\n",
    "                                 use_gpu=True,\n",
    "                                 embed_title=True,\n",
    "                                 use_fast_tokenizers=True,\n",
    "                                 similarity_function=\"cosine\")\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at ../Models/Context Models/Saved Models/roberta_base_squad2\n",
      "INFO - haystack.modeling.model.language_model -  Loaded ../Models/Context Models/Saved Models/roberta_base_squad2\n",
      "INFO - haystack.modeling.model.adaptive_model -  Found files for loading 1 prediction heads\n",
      "WARNING - haystack.modeling.model.prediction_head -  Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": false, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "INFO - haystack.modeling.model.prediction_head -  Loading prediction head from ..\\Models\\Context Models\\Saved Models\\roberta_base_squad2\\prediction_head_0.bin\n",
      "file ../Models/Context Models/Saved Models/roberta_base_squad2\\config.json not found\n",
      "INFO - haystack.modeling.data_handler.processor -  Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "reader = FARMReader(model_name_or_path=\"../Models/Context Models/Saved Models/roberta_base_squad2\", use_gpu=True,\n",
    "num_processes=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "216b33ba287e42d2a87342f3b9e368cf1b71a9244e6603550934e59b39d7eb84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
