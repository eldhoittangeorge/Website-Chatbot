{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline, AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline \n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from pymongo import MongoClient\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = MongoClient(host=\"localhost\", port=27017)\n",
    "db = db_client[\"Website_Chatbot\"]\n",
    "collection = db[\"MITS\"]\n",
    "\n",
    "questions = []\n",
    "for document in collection.find():\n",
    "    questions += document[\"questions\"]\n",
    "\n",
    "non_context_data = pd.read_csv(\"Data/data.csv\")\n",
    "context_data = questions\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "data = [{\"label\":0, \"text\":x[1].Question} for x in non_context_data.iterrows()]\n",
    "data += [{\"label\":1, \"text\":x} for x in context_data]\n",
    "data_df = pd.DataFrame(data, index=None)\n",
    "data_dataset = Dataset.from_pandas(data_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under Smapling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.array(data_df.index).reshape(11243, -1)\n",
    "labels = data_df.label\n",
    "over_sampler = RandomUnderSampler(random_state=46)\n",
    "x_resampled, y_resampled = over_sampler.fit_resample(text, labels)\n",
    "x_resampled = [data_df.text.iloc[x].item() for x in x_resampled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 25.64ba/s]\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.DataFrame({\"text\":x_resampled, \"label\":y_resampled})\n",
    "hf_dataset = Dataset.from_pandas(final_df)\n",
    "tokenized_data = hf_dataset.map(preprocess_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test, Validation split\n",
    "train_test = tokenized_data.train_test_split(test_size=.3)\n",
    "test_valid = train_test['test'].train_test_split(test_size=.4)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=.2)\n",
    "\n",
    "# x_train, x_validation , y_train, y_validation  = train_test_split(x_train, y_train, test_size=.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2030\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 870\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(len(x_train))\n",
    "# print(len(x_test))\n",
    "# print(len(x_validation))\n",
    "# test_valid\n",
    "train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = DatasetDict({\n",
    "    \"train\": train_test['train'],\n",
    "    \"test\": test_valid['train'],\n",
    "    \"valid\": test_valid['test']\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 27.18ba/s]\n"
     ]
    }
   ],
   "source": [
    "# y_resampled = list(y_resampled)\n",
    "# sampled_df = pd.DataFrame({\"label\": y_resampled, \"text\": x_resampled})\n",
    "# data_dataset = Dataset.from_pandas(sampled_df)\n",
    "# tokenized_data = data_dataset.map(preprocess_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2030\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 522\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 348\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "***** Running training *****\n",
      "  Num examples = 2030\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1270\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "                                       \n",
      "  0%|          | 0/635 [05:55<?, ?it/s]           Saving model checkpoint to ./results_undersampling\\checkpoint-500\n",
      "Configuration saved in ./results_undersampling\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 1.2125984251968505e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_undersampling\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_undersampling\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_undersampling\\checkpoint-500\\special_tokens_map.json\n",
      "                                       \n",
      "  0%|          | 0/635 [06:54<?, ?it/s]            Saving model checkpoint to ./results_undersampling\\checkpoint-1000\n",
      "Configuration saved in ./results_undersampling\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0022, 'learning_rate': 4.251968503937008e-06, 'epoch': 7.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_undersampling\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_undersampling\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_undersampling\\checkpoint-1000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "                                       \n",
      "100%|██████████| 1270/1270 [02:26<00:00,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 146.4722, 'train_samples_per_second': 138.593, 'train_steps_per_second': 8.671, 'train_loss': 0.0010962399104096758, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1270, training_loss=0.0010962399104096758, metrics={'train_runtime': 146.4722, 'train_samples_per_second': 138.593, 'train_steps_per_second': 8.671, 'train_loss': 0.0010962399104096758, 'epoch': 10.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_undersampling\",\n",
    "    learning_rate=2e-5,\n",
    "    per_gpu_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset['train'],\n",
    "    eval_dataset=final_dataset[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results_undersampling/checkpoint-1000/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"results_undersampling/checkpoint-1000/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file results_undersampling/checkpoint-1000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at results_undersampling/checkpoint-1000/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "under_sampled_trained_model = AutoModelForSequenceClassification.from_pretrained(\"results_undersampling/checkpoint-1000/\")\n",
    "under_sampled_pipeline = TextClassificationPipeline(model=under_sampled_trained_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = pipeline(final_dataset['test'])\n",
    "x_test = final_dataset['test']['text']\n",
    "y_test = final_dataset['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipeline(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [0 if x['label'] ==\"LABEL_0\" else 1 for x in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is 0.9980842911877394\n",
      "The precision score is 0.9961389961389961\n",
      "The recall score is 1.0\n",
      "The f1 score is 0.9980657640232108\n"
     ]
    }
   ],
   "source": [
    "print(f\"The accuracy score is {accuracy_score(y_test, prediction)}\")\n",
    "print(f\"The precision score is {precision_score(y_test, prediction)}\")\n",
    "print(f\"The recall score is {recall_score(y_test, prediction)}\")\n",
    "print(f\"The f1 score is {f1_score(y_test, prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.cuda.empty_cache()\n",
    "import mlflow\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.array(data_df.index).reshape(11243, -1)\n",
    "labels = data_df.label\n",
    "over_sampler = RandomOverSampler(random_state=46)\n",
    "x_resampled, y_resampled = over_sampler.fit_resample(text, labels)\n",
    "x_resampled = [data_df.text.iloc[x].item() for x in x_resampled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 19.49ba/s]\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.DataFrame({\"text\":x_resampled, \"label\":y_resampled})\n",
    "hf_dataset = Dataset.from_pandas(final_df)\n",
    "tokenized_data = hf_dataset.map(preprocess_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = tokenized_data.train_test_split(test_size=.3)\n",
    "test_valid = train_test['test'].train_test_split(test_size=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test\n",
    "# test_valid\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": train_test['train'],\n",
    "    \"test\": test_valid['train'],\n",
    "    \"valid\": test_valid['test']\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 13710\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3525\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2351\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "***** Running training *****\n",
      "  Num examples = 13710\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8570\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  6%|▌         | 500/8570 [00:59<14:46,  9.10it/s] Saving model checkpoint to ./results_oversampling\\checkpoint-500\n",
      "Configuration saved in ./results_oversampling\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0452, 'learning_rate': 1.8833138856476082e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-500\\special_tokens_map.json\n",
      " 12%|█▏        | 1000/8570 [01:58<12:55,  9.76it/s] Saving model checkpoint to ./results_oversampling\\checkpoint-1000\n",
      "Configuration saved in ./results_oversampling\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0077, 'learning_rate': 1.766627771295216e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-1000\\special_tokens_map.json\n",
      " 18%|█▊        | 1500/8570 [02:57<13:26,  8.77it/s]  Saving model checkpoint to ./results_oversampling\\checkpoint-1500\n",
      "Configuration saved in ./results_oversampling\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0032, 'learning_rate': 1.649941656942824e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-1500\\special_tokens_map.json\n",
      " 23%|██▎       | 2000/8570 [03:57<12:38,  8.66it/s]  Saving model checkpoint to ./results_oversampling\\checkpoint-2000\n",
      "Configuration saved in ./results_oversampling\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0019, 'learning_rate': 1.5332555425904317e-05, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-2000\\special_tokens_map.json\n",
      " 29%|██▉       | 2500/8570 [04:57<10:22,  9.75it/s]  Saving model checkpoint to ./results_oversampling\\checkpoint-2500\n",
      "Configuration saved in ./results_oversampling\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.4165694282380397e-05, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-2500\\special_tokens_map.json\n",
      " 35%|███▌      | 3000/8570 [05:56<09:40,  9.59it/s]  Saving model checkpoint to ./results_oversampling\\checkpoint-3000\n",
      "Configuration saved in ./results_oversampling\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.2998833138856476e-05, 'epoch': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-3000\\special_tokens_map.json\n",
      " 41%|████      | 3500/8570 [06:55<11:25,  7.40it/s]  Saving model checkpoint to ./results_oversampling\\checkpoint-3500\n",
      "Configuration saved in ./results_oversampling\\checkpoint-3500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.1831971995332557e-05, 'epoch': 4.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-3500\\special_tokens_map.json\n",
      " 47%|████▋     | 4000/8570 [07:54<08:15,  9.22it/s]  Saving model checkpoint to ./results_oversampling\\checkpoint-4000\n",
      "Configuration saved in ./results_oversampling\\checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.0665110851808636e-05, 'epoch': 4.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-4000\\special_tokens_map.json\n",
      " 53%|█████▎    | 4500/8570 [08:53<06:20, 10.69it/s]  Saving model checkpoint to ./results_oversampling\\checkpoint-4500\n",
      "Configuration saved in ./results_oversampling\\checkpoint-4500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 9.498249708284714e-06, 'epoch': 5.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-4500\\special_tokens_map.json\n",
      " 58%|█████▊    | 5000/8570 [09:52<05:42, 10.42it/s]  Saving model checkpoint to ./results_oversampling\\checkpoint-5000\n",
      "Configuration saved in ./results_oversampling\\checkpoint-5000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 8.331388564760793e-06, 'epoch': 5.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-5000\\special_tokens_map.json\n",
      " 64%|██████▍   | 5500/8570 [10:52<05:45,  8.88it/s]Saving model checkpoint to ./results_oversampling\\checkpoint-5500\n",
      "Configuration saved in ./results_oversampling\\checkpoint-5500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 7.164527421236873e-06, 'epoch': 6.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-5500\\special_tokens_map.json\n",
      " 70%|███████   | 6000/8570 [11:49<04:20,  9.86it/s]  Saving model checkpoint to ./results_oversampling\\checkpoint-6000\n",
      "Configuration saved in ./results_oversampling\\checkpoint-6000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 5.9976662777129524e-06, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-6000\\special_tokens_map.json\n",
      " 76%|███████▌  | 6500/8570 [12:49<04:37,  7.47it/s]Saving model checkpoint to ./results_oversampling\\checkpoint-6500\n",
      "Configuration saved in ./results_oversampling\\checkpoint-6500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 4.830805134189031e-06, 'epoch': 7.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-6500\\special_tokens_map.json\n",
      " 82%|████████▏ | 7000/8570 [13:46<03:13,  8.13it/s]Saving model checkpoint to ./results_oversampling\\checkpoint-7000\n",
      "Configuration saved in ./results_oversampling\\checkpoint-7000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 3.6639439906651113e-06, 'epoch': 8.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-7000\\special_tokens_map.json\n",
      " 88%|████████▊ | 7500/8570 [14:44<01:51,  9.63it/s]Saving model checkpoint to ./results_oversampling\\checkpoint-7500\n",
      "Configuration saved in ./results_oversampling\\checkpoint-7500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 2.4970828471411906e-06, 'epoch': 8.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-7500\\special_tokens_map.json\n",
      " 93%|█████████▎| 8000/8570 [15:43<01:04,  8.84it/s]Saving model checkpoint to ./results_oversampling\\checkpoint-8000\n",
      "Configuration saved in ./results_oversampling\\checkpoint-8000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.3302217036172696e-06, 'epoch': 9.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-8000\\special_tokens_map.json\n",
      " 99%|█████████▉| 8500/8570 [16:38<00:08,  8.70it/s]Saving model checkpoint to ./results_oversampling\\checkpoint-8500\n",
      "Configuration saved in ./results_oversampling\\checkpoint-8500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.633605600933489e-07, 'epoch': 9.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results_oversampling\\checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results_oversampling\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results_oversampling\\checkpoint-8500\\special_tokens_map.json\n",
      "100%|█████████▉| 8569/8570 [16:50<00:00,  8.16it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 8570/8570 [16:51<00:00,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1011.442, 'train_samples_per_second': 135.549, 'train_steps_per_second': 8.473, 'train_loss': 0.0033963261495195663, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8570, training_loss=0.0033963261495195663, metrics={'train_runtime': 1011.442, 'train_samples_per_second': 135.549, 'train_steps_per_second': 8.473, 'train_loss': 0.0033963261495195663, 'epoch': 10.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_oversampling\",\n",
    "    learning_rate=2e-5,\n",
    "    per_gpu_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset['train'],\n",
    "    eval_dataset=final_dataset[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results_oversampling/checkpoint-8500/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"results_oversampling/checkpoint-8500/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file results_oversampling/checkpoint-8500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at results_oversampling/checkpoint-8500/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "over_sampled_trained_model = AutoModelForSequenceClassification.from_pretrained(\"results_oversampling/checkpoint-8500/\")\n",
    "over_sampled_pipeline = TextClassificationPipeline(model=over_sampled_trained_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = final_dataset[\"test\"]['text']\n",
    "y_test = final_dataset[\"test\"]['label']\n",
    "\n",
    "prediction = pipeline(x_test)\n",
    "\n",
    "prediction = [0 if x['label'] == \"LABEL_0\" else 1 for x in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3525"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9999991655349731}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"who is the principal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is 0.9997163120567376\n",
      "The precision score is 0.9994353472614342\n",
      "The recall score is 1.0\n",
      "The f1 score is 0.9997175939000282\n"
     ]
    }
   ],
   "source": [
    "print(f\"The accuracy score is {accuracy_score(y_test, prediction)}\")\n",
    "print(f\"The precision score is {precision_score(y_test, prediction)}\")\n",
    "print(f\"The recall score is {recall_score(y_test, prediction)}\")\n",
    "print(f\"The f1 score is {f1_score(y_test, prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_world_question = [\"Who is the principal\", \"who is the principal of mits\",\n",
    "\"Good morning\", \"Who are you\", \"what is the eco club\", \n",
    "\"Where is the college located\", \"where is your location\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9999991655349731},\n",
       " {'label': 'LABEL_1', 'score': 0.999991774559021},\n",
       " {'label': 'LABEL_0', 'score': 0.9999994039535522},\n",
       " {'label': 'LABEL_0', 'score': 0.9999995231628418},\n",
       " {'label': 'LABEL_0', 'score': 0.999011754989624},\n",
       " {'label': 'LABEL_1', 'score': 0.8898599147796631},\n",
       " {'label': 'LABEL_0', 'score': 0.9999995231628418}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Over sampled model\n",
    "over_sampled_pipeline(real_world_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9999964237213135},\n",
       " {'label': 'LABEL_1', 'score': 0.999997615814209},\n",
       " {'label': 'LABEL_0', 'score': 0.9999982118606567},\n",
       " {'label': 'LABEL_0', 'score': 0.9999986886978149},\n",
       " {'label': 'LABEL_1', 'score': 0.9999985694885254},\n",
       " {'label': 'LABEL_1', 'score': 0.9999967813491821},\n",
       " {'label': 'LABEL_0', 'score': 0.9999982118606567}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_sampled_pipeline(real_world_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "216b33ba287e42d2a87342f3b9e368cf1b71a9244e6603550934e59b39d7eb84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
