{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ANT-PC\\anaconda3\\envs\\ds\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO - haystack.document_stores.base -  Numba not found, replacing njit() with no-op implementation. Enable it with 'pip install numba'.\n",
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
      "ERROR - root -  Failed to import 'magic' (from 'python-magic' and 'python-magic-bin' on Windows). FileTypeClassifier will not perform mimetype detection on extensionless files. Please make sure the necessary OS libraries are installed if you need this functionality.\n"
     ]
    }
   ],
   "source": [
    "from haystack.utils import   print_answers\n",
    "from haystack.nodes import FARMReader\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, PreProcessor\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from pymongo import MongoClient\n",
    "from haystack.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_client = MongoClient(host=\"localhost\", port=27017)\n",
    "database = db_client['Website_Chatbot']\n",
    "collection = database[\"MitsSpider\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [] \n",
    "for document in collection.find({}):\n",
    "    content = document[\"content\"]\n",
    "    for para in content.split(\"\\n\\n\"):\n",
    "        if not para.strip():\n",
    "            continue\n",
    "\n",
    "        tmp_doc = Document(content=para)\n",
    "        tmp_doc.id = str(document[\"_id\"])\n",
    "        tmp_doc.meta = {\"source\":document['source'], }\n",
    "        tmp_doc.content_type = \"str\"\n",
    "        docs.append(tmp_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_docs = [] \n",
    "for document in collection.find({}):\n",
    "    content = document[\"content\"]\n",
    "    for para in content.split(\"\\n\\n\"):\n",
    "        if not para.strip():\n",
    "            continue\n",
    "\n",
    "        tmp_doc = dict() #Document(content=para)\n",
    "        tmp_doc[\"content\"] = para\n",
    "        tmp_doc[\"id\"] = str(document[\"_id\"])\n",
    "        tmp_doc[\"meta\"] = {\"source\":document['source'],\"name\":str(document[\"_id\"]) }\n",
    "        # tmp_doc.content_type = \"str\"\n",
    "        new_docs.append(tmp_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4488/2622850787.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_header_footer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_by\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclean_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# preprocessor.clean(docs, clean_whitespace=True, clean_header_footer=True, clean_empty_lines=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\nodes\\preprocessor\\preprocessor.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, documents, clean_whitespace, clean_header_footer, clean_empty_lines, split_by, split_length, split_overlap, split_respect_sentence_boundary)\u001b[0m\n\u001b[0;32m    128\u001b[0m         )\n\u001b[0;32m    129\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             ret = self._process_batch(\n\u001b[0m\u001b[0;32m    131\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\nodes\\preprocessor\\preprocessor.py\u001b[0m in \u001b[0;36m_process_batch\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     ) -> List[dict]:\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mnested_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"docs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnested_docs\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\nodes\\preprocessor\\preprocessor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     ) -> List[dict]:\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mnested_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"docs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnested_docs\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\nodes\\preprocessor\\preprocessor.py\u001b[0m in \u001b[0;36m_process_single\u001b[1;34m(self, document, clean_whitespace, clean_header_footer, clean_empty_lines, split_by, split_length, split_overlap, split_respect_sentence_boundary)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0msplit_respect_sentence_boundary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_respect_sentence_boundary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         cleaned_document = self.clean(\n\u001b[0m\u001b[0;32m    168\u001b[0m             \u001b[0mdocument\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mclean_whitespace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclean_whitespace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\nodes\\preprocessor\\preprocessor.py\u001b[0m in \u001b[0;36mclean\u001b[1;34m(self, document, clean_whitespace, clean_header_footer, clean_empty_lines)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mIts\u001b[0m \u001b[0mexact\u001b[0m \u001b[0mfunctionality\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdefined\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0minto\u001b[0m \u001b[0mPreProcessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \"\"\"\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclean_header_footer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             text = self._find_and_remove_header_footer(\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "preprocessor = PreProcessor(clean_header_footer=True, split_by=None)\n",
    "clean_docs = preprocessor.process(docs)\n",
    "# preprocessor.clean(docs, clean_whitespace=True, clean_header_footer=True, clean_empty_lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing Documents: 10000it [00:00, 41031.36it/s]         \n"
     ]
    }
   ],
   "source": [
    "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")\n",
    "document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find facebook/dpr-question_encoder-single-nq-base locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded facebook/dpr-question_encoder-single-nq-base\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find facebook/dpr-ctx_encoder-single-nq-base locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded facebook/dpr-ctx_encoder-single-nq-base\n",
      "INFO - haystack.document_stores.faiss -  Updating embeddings for 98 docs...\n",
      "Updating Embedding:   0%|          | 0/98 [00:00<?, ? docs/s]C:\\Users\\ANT-PC\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\modeling\\data_handler\\dataset.py:65: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  cur_tensor = torch.tensor([sample[t_name] for sample in features], dtype=torch.long)\n",
      "Documents Processed: 10000 docs [00:10, 922.83 docs/s]        \n"
     ]
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(document_store=document_store,\n",
    "                                 query_embedding_model='facebook/dpr-question_encoder-single-nq-base',\n",
    "                                 passage_embedding_model='facebook/dpr-ctx_encoder-single-nq-base',\n",
    "                                 max_seq_len_query=64,\n",
    "                                 max_seq_len_passage=512,\n",
    "                                 batch_size=16,\n",
    "                                 use_gpu=True,\n",
    "                                 embed_title=True,\n",
    "                                 use_fast_tokenizers=True,\n",
    "                                 similarity_function=\"cosine\")\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at Saved Models/roberta_base_squad2\n",
      "INFO - haystack.modeling.model.language_model -  Loaded Saved Models/roberta_base_squad2\n",
      "INFO - haystack.modeling.model.adaptive_model -  Found files for loading 1 prediction heads\n",
      "WARNING - haystack.modeling.model.prediction_head -  Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": false, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "INFO - haystack.modeling.model.prediction_head -  Loading prediction head from Saved Models\\roberta_base_squad2\\prediction_head_0.bin\n",
      "WARNING - haystack.modeling.logger -  Failed to log params: Changing param values is not allowed. Param with key='prediction_heads' was already logged with value='TextSimilarityHead' for run ID='34280a31fc2e4691a5ceab7cae59598b'. Attempted logging new value 'QuestionAnsweringHead'.\n",
      "WARNING - haystack.modeling.logger -  Failed to log params: Changing param values is not allowed. Param with key='processor' was already logged with value='TextSimilarityProcessor' for run ID='34280a31fc2e4691a5ceab7cae59598b'. Attempted logging new value 'SquadProcessor'.\n",
      "INFO - haystack.modeling.data_handler.processor -  Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "reader = FARMReader(model_name_or_path=\"Saved Models/roberta_base_squad2\", use_gpu=True,\n",
    "num_processes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ExtractiveQAPipeline(reader=reader,retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.55 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Who is the principal of MITS\n",
      "Answers:\n",
      "[   <Answer {'answer': 'Dr Chikku Abraham', 'type': 'extractive', 'score': 0.6559404730796814, 'context': 'llege Executive Director Mr.P George Varghese, the acting Pricipal Dr Chikku Abraham and the HODs of various departments were the dignitaries on the d', 'offsets_in_document': [{'start': 3880, 'end': 3897}], 'offsets_in_context': [{'start': 67, 'end': 84}], 'document_id': '6213cd02abf8a36f04f5c6d4', 'meta': {'source': 'http://mgmits.ac.in/life-mits/sports/', 'vector_id': '32'}}>,\n",
      "    <Answer {'answer': 'Dr Chikku Abraham', 'type': 'extractive', 'score': 0.20968271791934967, 'context': 'Vice Principal Muthoot Institute of Technology and Science (MITS) Dr Chikku Abraham is currently the Vice- Principal and Associate Professor in Electr', 'offsets_in_document': [{'start': 66, 'end': 83}], 'offsets_in_context': [{'start': 66, 'end': 83}], 'document_id': '6213cd07abf8a36f04f5c750', 'meta': {'source': 'http://mgmits.ac.in/mits/executive-body/vice-principal/', 'vector_id': '93'}}>,\n",
      "    <Answer {'answer': 'Mr. George Alexander Muthoot', 'type': 'extractive', 'score': 0.08442884683609009, 'context': 's furnished with synthetic playing surface. Muthoot group MD Mr. George Alexander Muthoot inaugurated the indoor court on 12-01-2022. The inaugural ev', 'offsets_in_document': [{'start': 1841, 'end': 1869}], 'offsets_in_context': [{'start': 61, 'end': 89}], 'document_id': '6213cd02abf8a36f04f5c6d4', 'meta': {'source': 'http://mgmits.ac.in/life-mits/sports/', 'vector_id': '32'}}>,\n",
      "    <Answer {'answer': 'Mr. Anish P Rajan', 'type': 'extractive', 'score': 0.06417724303901196, 'context': 'he year 2019-’20, ‘SPARK 2K20’ has conducted on 25th January 2020. Mr. Anish P Rajan, member of Indian Cricket team for physicallychallenged was the c', 'offsets_in_document': [{'start': 2703, 'end': 2720}], 'offsets_in_context': [{'start': 67, 'end': 84}], 'document_id': '6213cd02abf8a36f04f5c6d4', 'meta': {'source': 'http://mgmits.ac.in/life-mits/sports/', 'vector_id': '32'}}>,\n",
      "    <Answer {'answer': 'Dr Abhilash Antony', 'type': 'extractive', 'score': 0.04530875198543072, 'context': 'mbursement of fee for presenting papers/attending conferences. 1. Dr Abhilash Antony -Faculty Chairman R&C (Asso.Professor -ECE) 2. Dr Rajesh Cherian ', 'offsets_in_document': [{'start': 1239, 'end': 1257}], 'offsets_in_context': [{'start': 66, 'end': 84}], 'document_id': '6213ccffabf8a36f04f5c693', 'meta': {'source': 'http://mgmits.ac.in/research/research-and-consultancy-cell/', 'vector_id': '0'}}>,\n",
      "    <Answer {'answer': 'Dr Gylson Thomas', 'type': 'extractive', 'score': 0.02031051553785801, 'context': 'CONTINUING EDUCATION CELL (CEC-MITS) Dr Gylson Thomas, Faculty-in-charge Continuing Education Cell, MITS For more information, please click Our major ', 'offsets_in_document': [{'start': 37, 'end': 53}], 'offsets_in_context': [{'start': 37, 'end': 53}], 'document_id': '6213cd03abf8a36f04f5c6f4', 'meta': {'source': 'http://mgmits.ac.in/research/continuing-education-cell/', 'vector_id': '48'}}>,\n",
      "    <Answer {'answer': 'ian Roy', 'type': 'extractive', 'score': 0.002832058467902243, 'context': 'ash Antony -Faculty Chairman R&C (Asso.Professor -ECE) 2. Dr Rajesh Cherian Roy(Professor-CSE) 3. Dr Renju Rajan -Secretary (Asst.Professor-BS&H) 4. M', 'offsets_in_document': [{'start': 1319, 'end': 1326}], 'offsets_in_context': [{'start': 72, 'end': 79}], 'document_id': '6213ccffabf8a36f04f5c693', 'meta': {'source': 'http://mgmits.ac.in/research/research-and-consultancy-cell/', 'vector_id': '0'}}>,\n",
      "    <Answer {'answer': 'Prof. Dr. Ashwin', 'type': 'extractive', 'score': 0.002172774344217032, 'context': 'ces, seminars and symposia of mutual interest to the institutions. Prof. Dr. Ashwin, faculty, GSU, USA interacts with ECE faculty Prof. Dr. Ashwin, fa', 'offsets_in_document': [{'start': 1048, 'end': 1064}], 'offsets_in_context': [{'start': 67, 'end': 83}], 'document_id': '6213cd06abf8a36f04f5c72d', 'meta': {'source': 'http://mgmits.ac.in/gsu/', 'vector_id': '76'}}>,\n",
      "    <Answer {'answer': 'Shri. Vinod Tharakan', 'type': 'extractive', 'score': 0.0018105313065461814, 'context': 'Part 1 (full) : Lecture 1 – Shri. Vinod Tharakan, CEO, Claysys : Lecture 2 – Shri. M. M. Murugappan, Chairman, Murugappa Group : Lecture 3 – Shri. G. ', 'offsets_in_document': [{'start': 28, 'end': 48}], 'offsets_in_context': [{'start': 28, 'end': 48}], 'document_id': '6213cd02abf8a36f04f5c6e6', 'meta': {'source': 'http://mgmits.ac.in/life-mits/distinguished-lectures/', 'vector_id': '41'}}>,\n",
      "    <Answer {'answer': 'Performer', 'type': 'extractive', 'score': 0.0013735965476371348, 'context': 'g of Institutions on Innovation Achievement (ARIIA) 2021, in the band “Performer” under the category “Colleges /Institutes (Private/Self Financed) (Te', 'offsets_in_document': [{'start': 110, 'end': 119}], 'offsets_in_context': [{'start': 71, 'end': 80}], 'document_id': '6213cd01abf8a36f04f5c6c2', 'meta': {'source': 'http://mgmits.ac.in/news-general/mits-has-been-recognised-in-ariia-2021-in-the-band-performer-under-the-category-colleges-institutes-private-self-financed-technical/', 'vector_id': '23'}}>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline.run(query=\"Who is the principal of MITS\",\n",
    "                         params = {\"Retriever\":{\"top_k\":10}, \n",
    "                                  \"Reader\":{\"top_k\":10}})\n",
    "\n",
    "print_answers(prediction)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.model.biadaptive_model -  prediction_head saving\n"
     ]
    }
   ],
   "source": [
    "retriever.save(\"context_model_retriever_2\")\n",
    "document_store.save(\"document_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at Saved Models/roberta_base_squad2\n",
      "INFO - haystack.modeling.model.language_model -  Loaded Saved Models/roberta_base_squad2\n",
      "INFO - haystack.modeling.model.adaptive_model -  Found files for loading 1 prediction heads\n",
      "WARNING - haystack.modeling.model.prediction_head -  Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": false, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "INFO - haystack.modeling.model.prediction_head -  Loading prediction head from Saved Models\\roberta_base_squad2\\prediction_head_0.bin\n",
      "INFO - haystack.modeling.data_handler.processor -  Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at context_model_retriever_2\\query_encoder\n",
      "INFO - haystack.modeling.model.language_model -  Loaded context_model_retriever_2\\query_encoder\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at context_model_retriever_2\\passage_encoder\n",
      "INFO - haystack.modeling.model.language_model -  Loaded context_model_retriever_2\\passage_encoder\n",
      "INFO - haystack.nodes.retriever.dense -  DPR model loaded from context_model_retriever_2\n"
     ]
    }
   ],
   "source": [
    "tmp_doc_store = FAISSDocumentStore.load(\"document_store\")\n",
    "tmp_reader = FARMReader(model_name_or_path=\"Saved Models/roberta_base_squad2\",\n",
    "use_gpu=True, num_processes=0)\n",
    "tmp_retriever = DensePassageRetriever.load(\"context_model_retriever_2\", tmp_doc_store)\n",
    "tmp_pipeline = ExtractiveQAPipeline(tmp_reader, tmp_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]C:\\Users\\ANT-PC\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\modeling\\model\\prediction_head.py:462: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  start_indices = flat_sorted_indices // max_seq_len\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.26s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "predictions = tmp_pipeline.run(\"Where is MITS located\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = predictions[\"answers\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6213cd05abf8a36f04f5c720'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0].document_id"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "216b33ba287e42d2a87342f3b9e368cf1b71a9244e6603550934e59b39d7eb84"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
