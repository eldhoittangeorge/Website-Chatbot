{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANT-PC\\anaconda3\\envs\\ds\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n",
      "C:\\Users\\ANT-PC\\anaconda3\\envs\\ds\\lib\\site-packages\\ray\\autoscaler\\_private\\cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from haystack.utils import  convert_files_to_dicts, print_answers\n",
    "from haystack.nodes import FARMReader\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from pymongo import MongoClient\n",
    "from haystack.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_client = MongoClient(host=\"localhost\", port=27017)\n",
    "database = db_client['Website_Chatbot']\n",
    "collection = database[\"MitsSpider\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paragraph(text):\n",
    "    print(text)\n",
    "    # for para in text.split(\"\\n\\n\"):\n",
    "    #     if not para.strip():\n",
    "    #         continue\n",
    "        # print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [] \n",
    "for document in collection.find({}):\n",
    "    content = document[\"content\"]\n",
    "    for para in content.split(\"\\n\\n\"):\n",
    "        if not para.strip():\n",
    "            continue\n",
    "\n",
    "        tmp_doc = Document(content=para)\n",
    "        tmp_doc.id = str(document[\"_id\"])\n",
    "        tmp_doc.meta = {\"source\":document['source'], }\n",
    "        tmp_doc.content_type = \"str\"\n",
    "        docs.append(tmp_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing Documents: 10000it [00:00, 37854.70it/s]         \n"
     ]
    }
   ],
   "source": [
    "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")\n",
    "document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find facebook/dpr-question_encoder-single-nq-base locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded facebook/dpr-question_encoder-single-nq-base\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find facebook/dpr-ctx_encoder-single-nq-base locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded facebook/dpr-ctx_encoder-single-nq-base\n",
      "INFO - haystack.document_stores.faiss -  Updating embeddings for 98 docs...\n",
      "Updating Embedding:   0%|          | 0/98 [00:00<?, ? docs/s]C:\\Users\\ANT-PC\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\modeling\\data_handler\\dataset.py:65: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  cur_tensor = torch.tensor([sample[t_name] for sample in features], dtype=torch.long)\n",
      "Documents Processed: 10000 docs [00:05, 1741.36 docs/s]       \n"
     ]
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(document_store=document_store,\n",
    "                                 query_embedding_model='facebook/dpr-question_encoder-single-nq-base',\n",
    "                                 passage_embedding_model='facebook/dpr-ctx_encoder-single-nq-base',\n",
    "                                 max_seq_len_query=64,\n",
    "                                 max_seq_len_passage=256,\n",
    "                                 batch_size=16,\n",
    "                                 use_gpu=True,\n",
    "                                 embed_title=True,\n",
    "                                 use_fast_tokenizers=True)\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at Saved Models/roberta_base_squad2\n",
      "INFO - haystack.modeling.model.language_model -  Loaded Saved Models/roberta_base_squad2\n",
      "INFO - haystack.modeling.model.adaptive_model -  Found files for loading 1 prediction heads\n",
      "WARNING - haystack.modeling.model.prediction_head -  Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": false, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "INFO - haystack.modeling.model.prediction_head -  Loading prediction head from Saved Models\\roberta_base_squad2\\prediction_head_0.bin\n",
      "WARNING - haystack.modeling.logger -  Failed to log params: Changing param values is not allowed. Param with key='prediction_heads' was already logged with value='TextSimilarityHead' for run ID='7072110055014d02a5f82777c3e2abe6'. Attempted logging new value 'QuestionAnsweringHead'.\n",
      "WARNING - haystack.modeling.logger -  Failed to log params: Changing param values is not allowed. Param with key='processor' was already logged with value='TextSimilarityProcessor' for run ID='7072110055014d02a5f82777c3e2abe6'. Attempted logging new value 'SquadProcessor'.\n",
      "INFO - haystack.modeling.data_handler.processor -  Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "reader = FARMReader(model_name_or_path=\"Saved Models/roberta_base_squad2\", use_gpu=True,\n",
    "num_processes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ExtractiveQAPipeline(reader=reader,retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]C:\\Users\\ANT-PC\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\modeling\\model\\prediction_head.py:462: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  start_indices = flat_sorted_indices // max_seq_len\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.07s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline.run(query=\"Where is mits located?\",\n",
    "                         params = {\"Retriever\":{\"top_k\":10}, \n",
    "                                  \"Reader\":{\"top_k\":10}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6213cd05abf8a36f04f5c720'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_ans = prediction[\"answers\"][0]\n",
    "first_ans.document_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.model.biadaptive_model -  prediction_head saving\n"
     ]
    }
   ],
   "source": [
    "retriever.save(\"context_model_retriever_2\")\n",
    "document_store.save(\"document_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at Saved Models/roberta_base_squad2\n",
      "INFO - haystack.modeling.model.language_model -  Loaded Saved Models/roberta_base_squad2\n",
      "INFO - haystack.modeling.model.adaptive_model -  Found files for loading 1 prediction heads\n",
      "WARNING - haystack.modeling.model.prediction_head -  Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": false, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "INFO - haystack.modeling.model.prediction_head -  Loading prediction head from Saved Models\\roberta_base_squad2\\prediction_head_0.bin\n",
      "INFO - haystack.modeling.data_handler.processor -  Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at context_model_retriever_2\\query_encoder\n",
      "INFO - haystack.modeling.model.language_model -  Loaded context_model_retriever_2\\query_encoder\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at context_model_retriever_2\\passage_encoder\n",
      "INFO - haystack.modeling.model.language_model -  Loaded context_model_retriever_2\\passage_encoder\n",
      "INFO - haystack.nodes.retriever.dense -  DPR model loaded from context_model_retriever_2\n"
     ]
    }
   ],
   "source": [
    "tmp_doc_store = FAISSDocumentStore.load(\"document_store\")\n",
    "tmp_reader = FARMReader(model_name_or_path=\"Saved Models/roberta_base_squad2\",\n",
    "use_gpu=True, num_processes=0)\n",
    "tmp_retriever = DensePassageRetriever.load(\"context_model_retriever_2\", tmp_doc_store)\n",
    "tmp_pipeline = ExtractiveQAPipeline(tmp_reader, tmp_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]C:\\Users\\ANT-PC\\anaconda3\\envs\\ds\\lib\\site-packages\\haystack\\modeling\\model\\prediction_head.py:462: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  start_indices = flat_sorted_indices // max_seq_len\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.26s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "predictions = tmp_pipeline.run(\"Where is MITS located\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = predictions[\"answers\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6213cd05abf8a36f04f5c720'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0].document_id"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5d4274ebb9180454c804b5ee419348493c013ff5d68b712df2e1ca9accf6ce7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
